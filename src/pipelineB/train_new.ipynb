{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee627c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current working directory: /cs/student/projects1/rai/2024/luttini/cw2_comp0248/src/pipelineB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"üìÅ Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5ba2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device name: Quadro K1200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check version\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, check the CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import cv2\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a3c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_intrinsics(intrinsics_path=None):\n",
    "    if intrinsics_path and os.path.exists(intrinsics_path):\n",
    "        with open(intrinsics_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        K = []\n",
    "        for line in lines:\n",
    "            nums = list(map(float, line.strip().split()))\n",
    "            K.append(nums)\n",
    "        K = np.array(K)\n",
    "        #print(f\"Loaded intrinsics from {intrinsics_path}:\")\n",
    "        #print(K)\n",
    "    else:\n",
    "        print(\"Using default intrinsics. Cannot load from file:\", intrinsics_path)\n",
    "        K = np.array([[570.3422, 0, 320],\n",
    "                      [0, 570.3422, 240],\n",
    "                      [0, 0, 1]])\n",
    "        #print(K)\n",
    "    return K\n",
    "\n",
    "def depth_to_pointcloud(depth_img, intrinsics):\n",
    "    fx, fy = intrinsics[0, 0], intrinsics[1, 1]\n",
    "    cx, cy = intrinsics[0, 2], intrinsics[1, 2]\n",
    "    height, width = depth_img.shape\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    Z = depth_img.astype(np.float32)\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "    pointcloud = np.stack((X, Y, Z), axis=-1).reshape(-1, 3)\n",
    "    valid = (Z.reshape(-1) > 0)\n",
    "    pointcloud = pointcloud[valid]\n",
    "    return pointcloud\n",
    "\n",
    "def downsample_pointcloud(pointcloud, num_points=1024):\n",
    "    N = pointcloud.shape[0]\n",
    "    if N >= num_points:\n",
    "        indices = np.random.choice(N, num_points, replace=False)\n",
    "    else:\n",
    "        indices = np.random.choice(N, num_points, replace=True)\n",
    "    return pointcloud[indices]\n",
    "\n",
    "def random_augmentation(sample):\n",
    "    # Augmentation function; can be set to None if not used.\n",
    "    pointcloud = sample[\"pointcloud\"].numpy()\n",
    "    angle = np.random.uniform(0, 2*np.pi)\n",
    "    R = np.array([[np.cos(angle), -np.sin(angle), 0],\n",
    "                  [np.sin(angle),  np.cos(angle), 0],\n",
    "                  [0,             0,              1]])\n",
    "    pointcloud = pointcloud @ R.T\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    pointcloud *= scale\n",
    "    jitter = np.random.normal(0, 0.01, pointcloud.shape)\n",
    "    pointcloud += jitter\n",
    "    sample[\"pointcloud\"] = torch.from_numpy(pointcloud).float()\n",
    "    return sample\n",
    "\n",
    "def has_table(polygon_list):\n",
    "    return len(polygon_list) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_scene_folder(predicted_scene_name, base_dataset_dir):\n",
    "    \"\"\"\n",
    "    Maps a predicted scene name from the depth PNG filename (subfolder name)\n",
    "    to the actual dataset folder that contains the labels and intrinsics.\n",
    "    For example:\n",
    "       If predicted_scene_name starts with \"76-\", then the real folder is:\n",
    "         os.path.join(base_dataset_dir, \"mit_76_studyroom\", predicted_scene_name)\n",
    "       If predicted_scene_name starts with \"d507\", then it is:\n",
    "         os.path.join(base_dataset_dir, \"mit_32_d507\", predicted_scene_name)\n",
    "    Modify this function based on your dataset organization.\n",
    "    \"\"\"\n",
    "    if predicted_scene_name.startswith(\"76-studyroom\"):\n",
    "        return os.path.join(base_dataset_dir, \"mit_76_studyroom\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"76-459\"):\n",
    "        return os.path.join(base_dataset_dir, \"mit_76_459\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"d507\"):\n",
    "        return os.path.join(base_dataset_dir, \"mit_32_d507\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"gym\"):\n",
    "        return os.path.join(base_dataset_dir, \"mit_gym_z_squash\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"lab\"):\n",
    "        return os.path.join(base_dataset_dir, \"mit_lab_hj\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"hv_tea2\"):\n",
    "        return os.path.join(base_dataset_dir, \"harvard_tea_2\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"hv_c11\"):\n",
    "        return os.path.join(base_dataset_dir, \"harvard_c11\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"hv_c6\"):\n",
    "        return os.path.join(base_dataset_dir, \"harvard_c6\", predicted_scene_name)\n",
    "    elif predicted_scene_name.startswith(\"hv_c5\"):\n",
    "        return os.path.join(base_dataset_dir, \"harvard_c5\", predicted_scene_name)\n",
    "    else:\n",
    "        return os.path.join(base_dataset_dir, predicted_scene_name)\n",
    "\n",
    "\n",
    "def get_real_scene_folder(predicted_scene_name, base_dataset_dir):\n",
    "    \"\"\"\n",
    "    Finds the full path of the dataset folder (e.g. 'mit_76_studyroom/76-1studyroom2')\n",
    "    given the subfolder name (e.g. '76-1studyroom2').\n",
    "    \"\"\"\n",
    "    for root, dirs, _ in os.walk(base_dataset_dir):\n",
    "        if predicted_scene_name in dirs:\n",
    "            return os.path.join(root, predicted_scene_name)\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Could not find folder '{predicted_scene_name}' inside base path: {base_dataset_dir}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PredictedDepthDataset(Dataset):\n",
    "    def __init__(self, depth_dir, base_dataset_dir, num_points=1024, transform=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depth_dir: Folder containing predicted depth PNG files. \n",
    "                       (e.g., \"data/depth_maps_PNG/train\")\n",
    "            base_dataset_dir: The base dataset directory where real scene folders reside.\n",
    "                              (e.g., \"data/mit_76_studyroom\" or \"data/mit_32_d507\")\n",
    "            num_points: Number of points to downsample each point cloud.\n",
    "            transform: Optional transform to apply to the sample.\n",
    "            verbose: If True, print extra debug info.\n",
    "        \"\"\"\n",
    "        self.depth_dir = depth_dir\n",
    "        self.base_dataset_dir = base_dataset_dir\n",
    "        self.num_points = num_points\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "        self.files = sorted([f for f in os.listdir(depth_dir) if f.lower().endswith('.png')])\n",
    "        self.label_cache = {}  # Cache for annotation files keyed by predicted scene name\n",
    "        if self.verbose:\n",
    "            print(f\"Loaded {len(self.files)} predicted depth PNG files from {depth_dir}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ---------------------------------------------\n",
    "        # 1. Load the predicted depth image\n",
    "        # ---------------------------------------------\n",
    "        depth_filename = self.files[idx]  # e.g., \"76-1studyroom2_0196_depth.png\"\n",
    "        depth_path = os.path.join(self.depth_dir, depth_filename)\n",
    "        depth_img = np.array(Image.open(depth_path)).astype(np.float32) / 1000.0  # Convert from mm to meters\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 2. Parse scene name and frame number from filename\n",
    "        # ---------------------------------------------\n",
    "        import re\n",
    "        match = re.match(r\"(.+?)_(\\d+)_depth\\.png\", depth_filename)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Filename '{depth_filename}' does not match expected pattern '<scene>_<frame>_depth.png'\")\n",
    "        predicted_scene_name = match.group(1)  # e.g., \"76-1studyroom2\"\n",
    "        \n",
    "        # ---------------------------------------------\n",
    "        # 3. Map to real dataset folder\n",
    "        # ---------------------------------------------\n",
    "        real_scene_folder = get_real_scene_folder(predicted_scene_name, self.base_dataset_dir)\n",
    "        if self.verbose:\n",
    "            print(f\"Mapping '{predicted_scene_name}' to real folder: {real_scene_folder}\")\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 4. Determine real frame index based on sorted depth files in real dataset\n",
    "        #    (Since frame numbers are from SUN and not contiguous)\n",
    "        # ---------------------------------------------\n",
    "        depth_folder = \"depth\" if \"harvard_tea_2\" in real_scene_folder else \"depthTSDF\"\n",
    "        real_depth_dir = os.path.join(real_scene_folder, depth_folder)\n",
    "        depth_filenames = sorted([f for f in os.listdir(real_depth_dir) if f.endswith('.png')])\n",
    "        \n",
    "        if idx < len(depth_filenames):\n",
    "            frame_index = idx  # Use index position as frame index\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Predicted frame index {idx} exceeds real dataset size ({len(depth_filenames)}).\")\n",
    "            frame_index = 0  # Fallback\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 5. Load intrinsics from file (or use default)\n",
    "        # ---------------------------------------------\n",
    "        intrinsics_file = os.path.join(real_scene_folder, \"intrinsics.txt\")\n",
    "        if os.path.exists(intrinsics_file):\n",
    "            K = load_intrinsics(intrinsics_file)\n",
    "        else:\n",
    "            K = np.array([[570.3422, 0, 320],\n",
    "                        [0, 570.3422, 240],\n",
    "                        [0, 0, 1]])\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 6. Convert depth map to point cloud and downsample\n",
    "        # ---------------------------------------------\n",
    "        pointcloud = depth_to_pointcloud(depth_img, K)\n",
    "        pointcloud = downsample_pointcloud(pointcloud, self.num_points)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 7. Load annotation (labels) and retrieve label\n",
    "        # ---------------------------------------------\n",
    "        annotation_file = os.path.join(real_scene_folder, \"labels\", \"tabletop_labels.dat\")\n",
    "        if predicted_scene_name not in self.label_cache:\n",
    "            if os.path.exists(annotation_file):\n",
    "                with open(annotation_file, 'rb') as f:\n",
    "                    self.label_cache[predicted_scene_name] = pickle.load(f)\n",
    "                if self.verbose:\n",
    "                    print(f\"Loaded annotations for {predicted_scene_name} with {len(self.label_cache[predicted_scene_name])} entries.\")\n",
    "            else:\n",
    "                self.label_cache[predicted_scene_name] = None\n",
    "                if self.verbose:\n",
    "                    print(f\"Warning: No annotation file found at {annotation_file}.\")\n",
    "\n",
    "        annotations = self.label_cache[predicted_scene_name]\n",
    "        if annotations is not None and frame_index < len(annotations):\n",
    "            polygons = annotations[frame_index]\n",
    "            label = int(has_table(polygons))  # Convert polygon list to binary label\n",
    "        else:\n",
    "            label = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Warning: Could not match annotation for frame index {frame_index} in {real_scene_folder}. Setting label to 0.\")\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 8. Return sample\n",
    "        # ---------------------------------------------\n",
    "        sample = {\n",
    "            \"pointcloud\": torch.from_numpy(pointcloud).float(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class TableClassificationDataset(Dataset):\n",
    "    def __init__(self, root_dir, depth_folder=\"\", annotation_path=None,\n",
    "                 intrinsics_path=None, num_points=1024, transform=None, verbose=False):\n",
    "        super().__init__()\n",
    "        if depth_folder:\n",
    "            self.depth_dir = os.path.join(root_dir, depth_folder)\n",
    "        else:\n",
    "            self.depth_dir = root_dir    # e.g., \"data/depth_maps_PNG/train\"\n",
    "\n",
    "        self.annotation_path = annotation_path\n",
    "        self.num_points = num_points\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Load PNG depth maps\n",
    "        self.depth_files = sorted([f for f in os.listdir(self.depth_dir) if f.lower().endswith('.png')])\n",
    "        if self.verbose:\n",
    "            print(f\"Found {len(self.depth_files)} PNG files in {self.depth_dir}\")\n",
    "\n",
    "        # If no annotation file, we'll just set label=0 for all\n",
    "        if annotation_path is None or not os.path.exists(os.path.join(root_dir, annotation_path)):\n",
    "            self.annotations = [None] * len(self.depth_files)\n",
    "        else:\n",
    "            # If you still want to load .dat annotations from somewhere else, handle it here\n",
    "            pass\n",
    "\n",
    "        # If intrinsics_path is None, use default intrinsics\n",
    "        self.intrinsics = load_intrinsics(intrinsics_path) if intrinsics_path else np.array([[570.3422, 0, 320],\n",
    "                                                                                            [0, 570.3422, 240],\n",
    "                                                                                            [0, 0, 1]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.depth_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        depth_path = os.path.join(self.depth_dir, self.depth_files[idx])\n",
    "        depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "        if depth_img is None:\n",
    "            raise FileNotFoundError(f\"Depth image not found: {depth_path}\")\n",
    "        depth_img = depth_img.astype(np.float32)\n",
    "\n",
    "        pointcloud = depth_to_pointcloud(depth_img, self.intrinsics)\n",
    "        pointcloud = downsample_pointcloud(pointcloud, self.num_points)\n",
    "\n",
    "        # If no annotation, default label=0 (or 1, or your logic)\n",
    "        label = 0\n",
    "\n",
    "        pointcloud = torch.from_numpy(pointcloud).float()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        sample = {\"pointcloud\": pointcloud, \"label\": label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6db5ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Depths:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/76-1studyroom2/depthTSDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Training Depths:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PredictedDepthDataset(\n\u001b[1;32m     10\u001b[0m     depth_dir\u001b[38;5;241m=\u001b[39mpredicted_train_dir,\n\u001b[1;32m     11\u001b[0m     base_dataset_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path),  \u001b[38;5;66;03m# where 'mit_*' and 'harvard_*' folders live\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m pos_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m neg_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m-\u001b[39m pos_count\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples ‚Äî \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positives, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneg_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m negatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Training Depths:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PredictedDepthDataset(\n\u001b[1;32m     10\u001b[0m     depth_dir\u001b[38;5;241m=\u001b[39mpredicted_train_dir,\n\u001b[1;32m     11\u001b[0m     base_dataset_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path),  \u001b[38;5;66;03m# where 'mit_*' and 'harvard_*' folders live\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m pos_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m neg_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m-\u001b[39m pos_count\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples ‚Äî \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positives, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneg_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m negatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 91\u001b[0m, in \u001b[0;36mPredictedDepthDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m depth_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mharvard_tea_2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m real_scene_folder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepthTSDF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m real_depth_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(real_scene_folder, depth_folder)\n\u001b[0;32m---> 91\u001b[0m depth_filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_depth_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(depth_filenames):\n\u001b[1;32m     94\u001b[0m     frame_index \u001b[38;5;241m=\u001b[39m idx  \u001b[38;5;66;03m# Use index position as frame index\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/76-1studyroom2/depthTSDF'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"../../data/\"\n",
    "    predicted_train_dir = os.path.join(base_path, \"depth_maps_PNG\", \"train\")\n",
    "    predicted_test_dir  = os.path.join(base_path, \"depth_maps_PNG\", \"test\")\n",
    "\n",
    "    # base_path = \"data/\"\n",
    "    # Train\n",
    "    print(\"Processing Training Depths:\")\n",
    "    train_dataset = PredictedDepthDataset(\n",
    "        depth_dir=predicted_train_dir,\n",
    "        base_dataset_dir=os.path.join(base_path),  # where 'mit_*' and 'harvard_*' folders live\n",
    "        num_points=1024,\n",
    "        transform=random_augmentation,\n",
    "        verbose=False\n",
    "    )\n",
    "    pos_count = sum(1 for i in range(len(train_dataset)) if train_dataset[i][\"label\"].item() == 1)\n",
    "    neg_count = len(train_dataset) - pos_count\n",
    "    print(f\"Train Dataset: {len(train_dataset)} samples ‚Äî {pos_count} positives, {neg_count} negatives.\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    print(\"Processing Test Depths:\")\n",
    "    test_dataset = PredictedDepthDataset(\n",
    "        depth_dir=predicted_test_dir,\n",
    "        base_dataset_dir=os.path.join(base_path),  # same logic\n",
    "        num_points=1024,\n",
    "        transform=random_augmentation,\n",
    "        verbose=False\n",
    "    )\n",
    "    pos_count = sum(1 for i in range(len(test_dataset)) if test_dataset[i][\"label\"].item() == 1)\n",
    "    neg_count = len(test_dataset) - pos_count\n",
    "    print(f\"Test Dataset: {len(test_dataset)} samples ‚Äî {pos_count} positives, {neg_count} negatives.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a052c1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4163668725.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helper functions for DGCNN\n",
    "# ------------------------\n",
    "\n",
    "def knn(x, k):\n",
    "    inner = -2 * torch.matmul(x.transpose(2, 1), x)  # (B, N, N)\n",
    "    xx = torch.sum(x ** 2, dim=1, keepdim=True)  # (B, 1, N)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)  # (B, N, N)\n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (B, N, k)\n",
    "    return idx\n",
    "\n",
    "def get_graph_feature(x, k=20, idx=None):\n",
    "    batch_size, num_dims, num_points = x.size()\n",
    "    if idx is None:\n",
    "        idx = knn(x, k=k)\n",
    "    device = x.device\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points\n",
    "    idx = idx + idx_base\n",
    "    idx = idx.view(-1)\n",
    "    x = x.transpose(2, 1).contiguous()  # (B, N, C)\n",
    "    feature = x.view(batch_size * num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    feature = torch.cat((feature - x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "    return feature\n",
    "\n",
    "# ------------------------\n",
    "# DGCNN Model Definition\n",
    "# ------------------------\n",
    "\n",
    "class DGCNNClassifier(nn.Module):\n",
    "    def __init__(self, k=20, emb_dims=1024, num_classes=2, dropout=0.5):\n",
    "        super(DGCNNClassifier, self).__init__()\n",
    "        self.k = k\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "            self.bn1,\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=1, bias=False),\n",
    "            self.bn2,\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1, bias=False),\n",
    "            self.bn3,\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=1, bias=False),\n",
    "            self.bn4,\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(512, emb_dims, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(emb_dims),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.linear1 = nn.Linear(emb_dims * 2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=dropout)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=dropout)\n",
    "        self.linear3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.transpose(2, 1)  # (B, 3, N)\n",
    "        x = get_graph_feature(x, k=self.k)  # (B, 6, N, k)\n",
    "        x = self.conv1(x)                   # (B, 64, N, k)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0] # (B, 64, N)\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k)  # (B, 128, N, k)\n",
    "        x = self.conv2(x)                   # (B, 64, N, k)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0] # (B, 64, N)\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k)  # (B, 128, N, k)\n",
    "        x = self.conv3(x)                   # (B, 128, N, k)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0] # (B, 128, N)\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k)  # (B, 256, N, k)\n",
    "        x = self.conv4(x)                   # (B, 256, N, k)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0] # (B, 256, N)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)  # (B, 512, N)\n",
    "        x = self.conv5(x)                     # (B, emb_dims, N)\n",
    "\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = torch.cat((x1, x2), 1)            # (B, emb_dims*2)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf81122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Training and Evaluation Functions\n",
    "# ------------------------\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        pointclouds = batch[\"pointcloud\"].to(device)  # (B, num_points, 3)\n",
    "        labels = batch[\"label\"].to(device)             # (B,)\n",
    "\n",
    "        if pointclouds.size(0) <= 1:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pointclouds)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * pointclouds.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    if total == 0:\n",
    "        return 0.0, 0.0\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pointclouds = batch[\"pointcloud\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            if pointclouds.size(0) <= 1:\n",
    "                continue\n",
    "\n",
    "            outputs = model(pointclouds)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * pointclouds.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    if total == 0:\n",
    "        return 0.0, 0.0, None, None\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_labels, all_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b89a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ------------------------\\n# Process Sequences Function\\n# ------------------------\\n\\ndef process_sequences(seq_list, base_path, set_name=\"Set\"):\\n    ds_list = []\\n    for seq in seq_list:\\n        seq_path = os.path.join(base_path, seq)\\n        if \"harvard_tea_2\" in seq:\\n            depth_folder = \"depth\"\\n        else:\\n            depth_folder = \"depthTSDF\"\\n        annotation_path = \"labels/tabletop_labels.dat\"\\n        intrinsics_path = os.path.join(seq_path, \"intrinsics.txt\")\\n        dataset = TableClassificationDataset(\\n            root_dir=seq_path,\\n            depth_folder=depth_folder,\\n            annotation_path=annotation_path,\\n            intrinsics_path=intrinsics_path,\\n            num_points=1024,\\n            transform=random_augmentation,\\n            verbose=False\\n        )\\n        pos_count = sum(1 for i in range(len(dataset)) if dataset[i][\"label\"].item() == 1)\\n        neg_count = len(dataset) - pos_count\\n        print(f\"{set_name} \\'{seq}\\' has {len(dataset)} PNG depth images: {pos_count} positives, {neg_count} negatives.\")\\n        ds_list.append(dataset)\\n    combined = ConcatDataset(ds_list)\\n    total_pos = sum(1 for i in range(len(combined)) if combined[i][\"label\"].item() == 1)\\n    total_neg = len(combined) - total_pos\\n    print(f\"{set_name} Combined dataset has {len(combined)} images: {total_pos} positives, {total_neg} negatives.\")\\n    return combined\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Process Sequences Function\n",
    "# ------------------------\n",
    "\n",
    "def process_sequences(seq_list, base_path, set_name=\"Set\"):\n",
    "    ds_list = []\n",
    "    for seq in seq_list:\n",
    "        seq_path = os.path.join(base_path, seq)  \n",
    "        # e.g. \"data/depth_maps_PNG/train\"\n",
    "\n",
    "        # We skip the old 'if \"harvard_tea_2\" in seq' logic\n",
    "        depth_folder = \"\"  # We'll pass an empty string so we read directly from seq_path\n",
    "        annotation_path = None  # We may not have annotation files in the new folder\n",
    "        intrinsics_path = None  # We can set a default intrinsics or skip\n",
    "\n",
    "        dataset = TableClassificationDataset(\n",
    "            root_dir=seq_path,\n",
    "            depth_folder=depth_folder,        # effectively the same folder\n",
    "            annotation_path=annotation_path,  # no table labels if not needed\n",
    "            intrinsics_path=intrinsics_path,  # or set a default\n",
    "            num_points=1024,\n",
    "            transform=random_augmentation,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        print(f\"{set_name} '{seq}' has {len(dataset)} PNG depth images.\")\n",
    "        ds_list.append(dataset)\n",
    "\n",
    "    combined = ConcatDataset(ds_list)\n",
    "    print(f\"{set_name} Combined dataset has {len(combined)} images total.\")\n",
    "    return combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "class PreprocessedPointCloudDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.files = sorted([f for f in os.listdir(root_dir) if f.endswith(\".npy\")])\n",
    "        self.root_dir = root_dir\n",
    "        print(f\"‚úÖ Loaded {len(self.files)} point clouds from {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = np.load(os.path.join(self.root_dir, self.files[idx]), allow_pickle=True).item()\n",
    "        pointcloud = torch.tensor(sample[\"points\"], dtype=torch.float32)  # [N, 3]\n",
    "        label = torch.tensor(sample[\"label\"], dtype=torch.long)           # 0 or 1\n",
    "        return {\n",
    "            \"pointcloud\": pointcloud,\n",
    "            \"label\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d7e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing Training Sequences:\n",
      "Total samples in train_full_dataset: 281\n",
      "Sample item: {'pointcloud': tensor([[-1.3456, -1.0092,  2.3983],\n",
      "        [-1.3083, -0.9843,  2.3392],\n",
      "        [-1.3612, -1.0273,  2.4413],\n",
      "        ...,\n",
      "        [ 0.4877,  0.3690,  1.4716],\n",
      "        [ 0.4903,  0.3690,  1.4716],\n",
      "        [ 0.4928,  0.3690,  1.4716]]), 'label': tensor(1)}\n",
      "\n",
      "Processing Test Sequences:\n",
      "\n",
      "Starting Fold 1/5\n",
      "Training size: 224 | Validation size: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/rai/2024/luttini/envs/COMP0248/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1152.00 GiB. GPU 0 has a total capacity of 3.94 GiB of which 3.59 GiB is free. Including non-PyTorch memory, this process has 337.75 MiB memory in use. Of the allocated memory 297.56 MiB is allocated by PyTorch, and 16.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cr)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 86\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m val_losses_fold \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 86\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     val_loss, val_acc, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m     88\u001b[0m     train_losses_fold\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointclouds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/cs/student/projects1/rai/2024/luttini/envs/COMP0248/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects1/rai/2024/luttini/envs/COMP0248/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36mDGCNNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 3, N)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, 6, N, k)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)                   \u001b[38;5;66;03m# (B, 64, N, k)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# (B, 64, N)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mget_graph_feature\u001b[0;34m(x, k, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m batch_size, num_dims, num_points \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     29\u001b[0m idx_base \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_points\n",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m, in \u001b[0;36mknn\u001b[0;34m(x, k)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mknn\u001b[39m(x, k):\n\u001b[0;32m---> 18\u001b[0m     inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, N, N)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# (B, 1, N)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     pairwise_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mxx \u001b[38;5;241m-\u001b[39m inner \u001b[38;5;241m-\u001b[39m xx\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, N, N)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1152.00 GiB. GPU 0 has a total capacity of 3.94 GiB of which 3.59 GiB is free. Including non-PyTorch memory, this process has 337.75 MiB memory in use. Of the allocated memory 297.56 MiB is allocated by PyTorch, and 16.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Main Training Script with 5-Fold Cross Validation (using StratifiedKFold) and ReduceLROnPlateau\n",
    "# and final evaluation with confusion matrix on the Test Set\n",
    "# ------------------------\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 8\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 1e-4  # L2 regularization\n",
    "    k_val = 20\n",
    "    emb_dims = 1024\n",
    "    dropout = 0.5\n",
    "    num_classes = 2\n",
    "    n_splits = 5\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # base_path = \"CW2-Dataset/data/\"\n",
    "    # sequences_train = [\n",
    "    #     \"mit_32_d507/d507_2\",\n",
    "    #     \"mit_76_459/76-459b\",\n",
    "    #     \"mit_76_studyroom/76-1studyroom2\",\n",
    "    #     \"mit_gym_z_squash/gym_z_squash_scan1_oct_26_2012_erika\",\n",
    "    #     \"mit_lab_hj/lab_hj_tea_nov_2_2012_scan1_erika\"\n",
    "    # ]\n",
    "    # sequences_test = [\n",
    "    #     \"harvard_c5/hv_c5_1\",\n",
    "    #     \"harvard_c6/hv_c6_1\",\n",
    "    #     \"harvard_c11/hv_c11_2\",\n",
    "    #     \"harvard_tea_2/hv_tea2_2\"\n",
    "    # ]\n",
    "\n",
    "    # print(\"Processing Training Sequences:\")\n",
    "    # # train_full_dataset = process_sequences(sequences_train, base_path, set_name=\"Train\")\n",
    "    # train_full_dataset = PreprocessedPointCloudDataset(\"../../data/pointclouds/train\")\n",
    "    train_full_dataset = process_sequences([\"train\"], base_path=\"data/depth_maps_PNG\", set_name=\"Train\")\n",
    "    print(f\"Total samples in train_full_dataset: {len(train_full_dataset)}\")\n",
    "\n",
    "    # Try printing a few samples if non-empty\n",
    "    if len(train_full_dataset) > 0:\n",
    "        print(\"Sample item:\", train_full_dataset[0])\n",
    "    else:\n",
    "        print(\"üö® ERROR: Your train dataset is empty!\")\n",
    "\n",
    "    print(\"\\nProcessing Test Sequences:\")\n",
    "    # test_dataset = process_sequences(sequences_test, base_path, set_name=\"Test\")\n",
    "    # test_dataset = PreprocessedPointCloudDataset(\"../../data/pointclouds/test\")\n",
    "    test_dataset  = process_sequences([\"test\"],  base_path=\"data/depth_maps_PNG\", set_name=\"Test\")\n",
    "\n",
    "    # Create label array for StratifiedKFold\n",
    "    labels = [train_full_dataset[i][\"label\"].item() for i in range(len(train_full_dataset))]\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_val_accs = []\n",
    "    fold_val_losses = []\n",
    "    best_fold_model_state = None\n",
    "    best_fold_val_acc = 0.0\n",
    "    all_fold_train_losses = []\n",
    "    all_fold_val_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "        print(f\"\\nStarting Fold {fold+1}/{n_splits}\")\n",
    "        print(f\"Training size: {len(train_idx)} | Validation size: {len(val_idx)}\")\n",
    "        train_subset = Subset(train_full_dataset, train_idx)\n",
    "        val_subset = Subset(train_full_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        model = DGCNNClassifier(k=k_val, emb_dims=emb_dims, num_classes=num_classes, dropout=dropout)\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # Use ReduceLROnPlateau to adjust learning rate when validation loss plateaus\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "        best_val_acc_fold = 0.0\n",
    "        best_val_loss_fold = float('inf')\n",
    "        train_losses_fold = []\n",
    "        val_losses_fold = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "            train_losses_fold.append(train_loss)\n",
    "            val_losses_fold.append(val_loss)\n",
    "            print(f\"Fold {fold+1} Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
    "            scheduler.step(val_loss)\n",
    "            if val_acc > best_val_acc_fold:\n",
    "                best_val_acc_fold = val_acc\n",
    "                best_val_loss_fold = val_loss\n",
    "                best_model_state_fold = model.state_dict()\n",
    "\n",
    "        print(f\"Fold {fold+1} Best Val Acc: {best_val_acc_fold:.4f} with Loss: {best_val_loss_fold:.4f}\")\n",
    "        fold_val_accs.append(best_val_acc_fold)\n",
    "        fold_val_losses.append(best_val_loss_fold)\n",
    "        all_fold_train_losses.append(train_losses_fold)\n",
    "        all_fold_val_losses.append(val_losses_fold)\n",
    "\n",
    "        if best_val_acc_fold > best_fold_val_acc:\n",
    "            best_fold_val_acc = best_val_acc_fold\n",
    "            best_fold_model_state = best_model_state_fold\n",
    "\n",
    "        # Plot loss curves for this fold\n",
    "        epochs_arr = np.arange(1, num_epochs + 1)\n",
    "        plt.figure()\n",
    "        plt.plot(epochs_arr, train_losses_fold, label=\"Train Loss\")\n",
    "        plt.plot(epochs_arr, val_losses_fold, label=\"Val Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Fold {fold+1} Loss Curves\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nCross Validation Complete.\")\n",
    "    print(f\"Average Validation Accuracy over {n_splits} folds: {np.mean(fold_val_accs):.4f}\")\n",
    "    print(f\"Best Fold Validation Accuracy: {best_fold_val_acc:.4f}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(np.arange(1, n_splits+1), fold_val_losses, tick_label=np.arange(1, n_splits+1))\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Best Validation Loss\")\n",
    "    plt.title(\"Best Validation Loss per Fold\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(best_fold_model_state, \"best_dgcnn_model.pth\")\n",
    "    print(\"Best model saved to 'best_dgcnn_model.pth'\")\n",
    "\n",
    "\n",
    "    # Evaluate best model on the test set\n",
    "    model = DGCNNClassifier(k=k_val, emb_dims=emb_dims, num_classes=num_classes, dropout=dropout)\n",
    "    model.load_state_dict(torch.load(\"best_dgcnn_model.pth\"))\n",
    "    model = model.to(device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loss, test_acc, y_true, y_pred = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(cr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
